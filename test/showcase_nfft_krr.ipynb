{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kernel Ridge Regression Learning for Large-Scale Data Using ANOVA-Based Matrix-Vector Multiplication**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Problem Setting**\n",
    "\n",
    "Solving the kernel ridge regression learning task\n",
    "$$\n",
    "\\hat{\\mathbf{\\alpha}} = \\underset{\\mathbf{\\alpha} \\in \\mathbb{R}^N}{\\arg \\text{min}} \\| \\mathbf{y} - K \\mathbf{\\alpha} \\|_2^2 + \\beta \\mathbf{\\alpha}^\\top  K \\mathbf{\\alpha},\n",
    "$$\n",
    "where $\\mathbf{y} = \\begin{bmatrix} {y}_{1}, \\dots, {y}_{N} \\end{bmatrix}^\\top$ is the target vector, $K = \\left( \\kappa_{ij} \\right)_{i,j=1}^N$ is the kernel matrix and $\\beta$ is the regularization parameter, is equivalent to solving the linear system\n",
    "$$\n",
    "\\left( K + \\beta I \\right) \\hat{\\mathbf{\\alpha}} = \\mathbf{y}.\n",
    "$$\n",
    "For $\\beta > 0$, this system is symmetric and positive definite and is solved with the CG-method, where multiplying with the kernel matrix $K$\n",
    "$$\n",
    "K \\bf{\\alpha} = \\left[\\sum_{j=1}^N \\alpha_j\\kappa\\left(\\bf{x}_i,\\bf{x}_j\\right)\\right]_{i=1}^N \\in\\mathbb{R}^N\n",
    "$$\n",
    "is the most expensive step, since computing one matrix-vector product is required in every iteration of the CG-method.\n",
    "\n",
    "We propose to not solve this directly but to approximate $K \\bf{\\alpha}$. For this, we make use of the *extended Gaussian ANOVA kernel*\n",
    "$$\n",
    "K = \\left( \\kappa_{ij} \\right)_{i,j = 1}^{N} \\in \\mathbb{R}^{N \\times N}, \\quad \\kappa_{ij} = \\sum_{l=1}^P \\frac{1}{P} \\exp \\left( - \\frac{ \\| \\bf{x}_{i}^{\\mathcal{W}_l} - \\bf{x} _{j}^{\\mathcal{W}_l} \\|_2^2}{\\sigma^2} \\right),\n",
    "$$\n",
    "where $\\sigma$ is a shape parameter, $P$ is the number of kernels to combine and $\\mathcal{W}_l = \\{ w_1^l, w_2^l, w_3^l \\} \\in \\{ 1, \\dots, d \\}^3$ are the considered index sets, so that $\\bf{x}_i^{\\mathcal{W}_l}$ and $\\bf{x}_j^{\\mathcal{W}_l}$ are the data points restricted to the corresponding features. By this, we have a weighted sum of multiple kernels, where every kernel relies on not more than 3 features and, thus, we can apply the NFFT-based fast summation approach and use the [`fastadj`](https://github.com/dominikalfke/FastAdjacency) package by Dominik Alfke to speed up the kernel-vector multiplication.\n",
    "\n",
    "First, we load the data set, we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N, d = 20000, 15\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(N, d)\n",
    "y = np.sign(rng.randn(N))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compare NFFTKernelRidge Classifier with sklearn KRR and sklearn SVC**\n",
    "\n",
    "We set up NFFT-based kernel ridge regression with the `NFFTKernelRidge` class. The second class `GridSearch` enables us to perform GridSearch for the NFFTKernelRidge classifier and the sklearn classifiers KRR and SVC.\n",
    "\n",
    "For this, we must define a `param_grid` with candidate parameter values, which shall be tried within GridSearch. In the following example, the grids for all classifiers are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GridSearch for NFFTKernelRidge\n",
      "Best Parameters: (0.001, 1000)\n",
      "Best Result: (0.5052, 0.5013201901073755, 0.5738464638323595)\n",
      "Best Runtime Fit: 1.2237987518310547\n",
      "Best Runtime Predict: 0.18867278099060059\n",
      "Mean Runtime Fit: 1.8293311425617762\n",
      "Mean Runtime Predict: 0.20490669352667673\n",
      "Mean Total Runtime: 2.034237836088453\n",
      "\n",
      "GridSearch for sklearn KRR\n",
      "Best Parameters: (100, 0.01)\n",
      "Best Result: (0.5019, 0.49855815443768026, 0.6270400967156962)\n",
      "Best Runtime Fit: 4.055310964584351\n",
      "Best Runtime Predict: 1.1931085586547852\n",
      "Mean Runtime Fit: 4.183920519692557\n",
      "Mean Runtime Predict: 1.4633090921810694\n",
      "Mean Total Runtime: 5.647229611873627\n",
      "\n",
      "GridSearch for sklearn SVC\n",
      "Best Parameters: (1.0, 0.01)\n",
      "Best Result: (0.4998, 0.4971702220287331, 0.690308281281483)\n",
      "Best Runtime Fit: 2.1764395236968994\n",
      "Best Runtime Predict: 1.602280616760254\n",
      "Mean Runtime Fit: 3.8639715228761946\n",
      "Mean Runtime Predict: 2.420388732637678\n",
      "Mean Total Runtime: 6.284360255513873\n"
     ]
    }
   ],
   "source": [
    "# import functions from extracted files\n",
    "from nfft_kernel_ridge import NFFTKernelRidge, GridSearch\n",
    "\n",
    "## GridSearch with NFFTKernelRidge\n",
    "param_grid = {\n",
    "    \"sigma\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"beta\": [1, 10, 100, 1000],\n",
    "}\n",
    "\n",
    "model = GridSearch(classifier=\"NFFTKernelRidge\", param_grid=param_grid, balance=False, norm=None)\n",
    "results = model.tune(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nGridSearch for NFFTKernelRidge\")\n",
    "print(\"Best Parameters:\", results[0])\n",
    "print(\"Best Result:\", results[1])\n",
    "print(\"Best Runtime Fit:\", results[2])\n",
    "print(\"Best Runtime Predict:\", results[3])\n",
    "print(\"Mean Runtime Fit:\", results[4])\n",
    "print(\"Mean Runtime Predict:\", results[5])\n",
    "print(\"Mean Total Runtime:\", results[6])\n",
    "\n",
    "\n",
    "## GridSearch with sklearn KRR\n",
    "param_grid = {\n",
    "    \"alpha\": [1, 10, 100, 1000],\n",
    "    \"gamma\": [1/((0.001)**2), 1/((0.01)**2), 1/((0.1)**2), 1/((1)**2), 1/((10)**2), 1/((100)**2), 1/((1000)**2)],\n",
    "}\n",
    "\n",
    "model = GridSearch(classifier=\"sklearn KRR\", param_grid=param_grid)\n",
    "results = model.tune(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nGridSearch for sklearn KRR\")\n",
    "print(\"Best Parameters:\", results[0])\n",
    "print(\"Best Result:\", results[1])\n",
    "print(\"Best Runtime Fit:\", results[2])\n",
    "print(\"Best Runtime Predict:\", results[3])\n",
    "print(\"Mean Runtime Fit:\", results[4])\n",
    "print(\"Mean Runtime Predict:\", results[5])\n",
    "print(\"Mean Total Runtime:\", results[6])\n",
    "\n",
    "\n",
    "## GridSearch with sklearn SVC\n",
    "param_grid = {\n",
    "    \"C\": [1/1, 1/10, 1/100, 1/1000], # (C = 1/alpha)\n",
    "    \"gamma\": [1/((0.001)**2), 1/((0.01)**2), 1/((0.1)**2), 1/((1)**2), 1/((10)**2), 1/((100)**2), 1/((1000)**2)],\n",
    "}\n",
    "\n",
    "model = GridSearch(classifier=\"sklearn SVC\", param_grid=param_grid)\n",
    "results = model.tune(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nGridSearch for sklearn SVC\")\n",
    "print(\"Best Parameters:\", results[0])\n",
    "print(\"Best Result:\", results[1])\n",
    "print(\"Best Runtime Fit:\", results[2])\n",
    "print(\"Best Runtime Predict:\", results[3])\n",
    "print(\"Mean Runtime Fit:\", results[4])\n",
    "print(\"Mean Runtime Predict:\", results[5])\n",
    "print(\"Mean Total Runtime:\", results[6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
